{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning, Preprocessing and EDA for Bank Customer Churn Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df = pd.read_csv('Customer-Churn-Records.csv')\n",
    "# Check the number of rows and columns\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Null Values & Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the counts in the desired format\n",
    "for dtype, count in  df.dtypes.value_counts().items():\n",
    "    print(f\"Columns of datatype {dtype}: {count}\")\n",
    "\n",
    "# check number of columns with null values\n",
    "null_counts = df.isnull().sum()\n",
    "print(f\"Total number of columns with null values: {len((null_counts[null_counts > 0]).tolist())}/{len(df.columns.tolist())}\")\n",
    "\n",
    "# Create DataFrame with column names, data types, and null counts\n",
    "column_info = pd.DataFrame({\n",
    "    \"Column Name\": df.columns,\n",
    "    \"Data Type\": [df[col].dtype for col in df.columns],\n",
    "    \"Null Counts\":  df.isnull().sum().values\n",
    "})\n",
    "column_info.index = column_info.index + 1\n",
    "column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all data to numerical format for data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns of object data type only\n",
    "object_columns = df.select_dtypes(include='object').columns\n",
    "\n",
    "# check the unique values and their total number for each object column\n",
    "for col in object_columns:\n",
    "    unique_values = df[col].unique()\n",
    "    print(f\"Unique values in '{col}': {unique_values}\")\n",
    "    total_num_of_unique_values =  df[col].nunique()\n",
    "    print(f\"Total number of Unique values in '{col}': {total_num_of_unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first drop the Surname and Row Number columns from the DataFrame\n",
    "df = df.drop(columns=['Surname', \"RowNumber\"])\n",
    "\n",
    "# verify Surname column is removed\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# use label encoding for Gender column as it contains categorical values that is not \n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "\n",
    "# use One-Hot encoding Geography column since there is no particular orders in the value\n",
    "df = pd.get_dummies(df, columns=['Geography'])\n",
    "\n",
    "card_type_mapping = {\n",
    "    'SILVER': 1, # lowest in rank\n",
    "    'GOLD': 2,\n",
    "    'PLATINUM': 3,\n",
    "    'DIAMOND': 4  # highest in rank\n",
    "}\n",
    "df['Card Type'] = df['Card Type'].map(card_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify the data transformation\n",
    "column_data_types = pd.DataFrame(df.dtypes, columns=['Data Type']).reset_index()\n",
    "column_data_types.columns = ['Column Name', 'Data Type']  # Rename columns\n",
    "\n",
    "column_data_types.index = column_data_types.index + 1\n",
    "column_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Boolean values to 1 and 0 in Geography columns (uncomment the below if your need to use this line of code)\n",
    "# df[['Geography_France', 'Geography_Germany', 'Geography_Spain']] = df[['Geography_France', 'Geography_Germany', 'Geography_Spain']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns of int and float data type only\n",
    "int_and_float_columns_df = df[df.select_dtypes(include=['int64', 'float64']).columns]\n",
    "\n",
    "# check their value range \n",
    "int_and_float_columns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of interest\n",
    "columns_to_scale = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary', 'NumOfProducts', 'Point Earned']\n",
    "\n",
    "# Get summary statistics for these columns\n",
    "summary_stats = df[columns_to_scale].describe().loc[['min', 'max', 'mean', 'std']]\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store outlier counts for each column\n",
    "outlier_counts = {}\n",
    "\n",
    "for column in columns_to_scale:\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    outlier_counts[column] = len(outliers)\n",
    "\n",
    "# Display the count of outliers in each column\n",
    "print(outlier_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply winsorizing method for Number of product column\n",
    "df['NumOfProducts'] = np.clip(df['NumOfProducts'], df['NumOfProducts'].quantile(0.05), df['NumOfProducts'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for CreditScore & filter out the rows where Z-score is greater than the threshold\n",
    "df['CreditScore_Z'] = (df['CreditScore'] - df['CreditScore'].mean()) / df['CreditScore'].std()\n",
    "threshold = 3\n",
    "df_filtered = df[np.abs(df['CreditScore_Z']) <= threshold]\n",
    "\n",
    "# Drop the temporary Z-score column\n",
    "df_filtered = df_filtered.drop(columns=['CreditScore_Z'])\n",
    "\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'Age' and 'Exited'\n",
    "correlation_age_exited = df_filtered['Age'].corr(df_filtered['Exited'])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Correlation between Age and Exited: {correlation_age_exited}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IQR bounds for the Age column to identify outliers\n",
    "Q1 = df_filtered['Age'].quantile(0.25)  \n",
    "Q3 = df_filtered['Age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter rows where Age is considered an outlier\n",
    "age_outliers = df_filtered[(df_filtered['Age'] < lower_bound) | (df_filtered['Age'] > upper_bound)]\n",
    "\n",
    "# Calculate the proportion of Exited vs. Non-Exited among Age outliers\n",
    "age_outliers_exited_counts = age_outliers['Exited'].value_counts()\n",
    "total_outliers = len(age_outliers)\n",
    "exited_proportion_in_outliers = age_outliers_exited_counts / total_outliers\n",
    "\n",
    "print(\"\\nProportion of 'Exited' within Age outliers:\")\n",
    "print(exited_proportion_in_outliers)\n",
    "\n",
    "# Get the count of each class in the Exited column for comparison\n",
    "class_counts = df_filtered['Exited'].value_counts()  \n",
    "total_number_of_samples = len(df_filtered)  \n",
    "exited_proportion_overall = class_counts / total_number_of_samples\n",
    "\n",
    "print(\"\\nOverall Proportion of 'Exited':\")\n",
    "print(exited_proportion_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lower and upper percentiles for capping\n",
    "lower_percentile = 1\n",
    "upper_percentile = 99\n",
    "\n",
    "# Calculate the lower and upper bounds using the percentiles\n",
    "lower_bound = np.percentile(df_filtered['Age'], lower_percentile)\n",
    "upper_bound = np.percentile(df_filtered['Age'], upper_percentile)\n",
    "\n",
    "# Apply capping by replacing values below lower bound with the lower bound & values aboveupper bound with the upper bound\n",
    "df_filtered['Age'] = np.where(df_filtered['Age'] < lower_bound, lower_bound, df_filtered['Age'])\n",
    "df_filtered['Age'] = np.where(df_filtered['Age'] > upper_bound, upper_bound, df_filtered['Age'])\n",
    "\n",
    "print(df_filtered['Age'].describe())\n",
    "\n",
    "# verify that the proportion of Exited vs. Non-Exited remains at ~ 80/20 split after the scaling\n",
    "class_counts = df_filtered['Exited'].value_counts()  \n",
    "total_number_of_samples = len(df_filtered)  \n",
    "exited_proportion_overall = class_counts / total_number_of_samples\n",
    "\n",
    "print(\"\\nOverall Proportion of 'Exited':\")\n",
    "print(exited_proportion_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the numerical features\n",
    "correlation_matrix = df_filtered.corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features (e.g., with a correlation coefficient > 0.8 or < -0.8)\n",
    "threshold = 0.8\n",
    "highly_correlated = np.where((correlation_matrix > threshold) | (correlation_matrix < -threshold))\n",
    "\n",
    "# Extract the indices of highly correlated features (excluding the diagonal)\n",
    "highly_correlated_pairs = [(correlation_matrix.columns[x], correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) \n",
    "                           if x != y and x < y]  # to avoid duplicate pairs\n",
    "\n",
    "# Display highly correlated feature pairs\n",
    "print(\"\\nHighly Correlated Feature Pairs (correlation > 0.8 or < -0.8):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportions of each class for the target variables\n",
    "class_proportions = df_filtered['Exited'].value_counts(normalize=True) * 100\n",
    "print(class_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Define the predicting variable & the target variable\n",
    "X = df_filtered.drop(columns=['Exited'])  \n",
    "y = df_filtered['Exited']                 \n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE only to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verify the class distribution\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Processed Data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames and save as CSVs\n",
    "train_df = pd.concat([X_train_smote, y_train_smote], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_df.to_csv(\"bank_customer_churn_train_data_processed.csv\", index=False)\n",
    "test_df.to_csv(\"bank_customer_churn_test_data_processed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer_churn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
